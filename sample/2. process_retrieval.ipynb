{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e413bb64-9eea-4b02-815c-c1e2d163a786",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac005560-82b1-44d3-b6b2-76cd01269c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\EO\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from tqdm import tqdm \n",
    "from sample.retrieval import generate_equal_kernel, generate_gaussian_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9349d6-fd4a-46d1-89ae-25ba27a062bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be21fb1-742f-4895-aa71-7c8c026e4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude Kernel (size=4, resolution=0.1): [-0.30000000000000004, -0.1, 0.1, 0.30000000000000004]\n",
      "Longitude Kernel (size=5, resolution=0.1): [-0.4, -0.2, 0.0, 0.2, 0.4]\n",
      "Temporal Kernel (size=5, mean=0, std_dev=15): [datetime.timedelta(days=-1, seconds=34159, microseconds=235431), datetime.timedelta(days=-1, seconds=63140, microseconds=725838), datetime.timedelta(0), datetime.timedelta(seconds=23259, microseconds=274162), datetime.timedelta(seconds=52240, microseconds=764569)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from tqdm import tqdm \n",
    "from sample.retrieval import generate_equal_kernel, generate_gaussian_kernel\n",
    "\n",
    "# Testing the equal kernel generation\n",
    "# The goal is to demonstrate how to create kernels (arrays of values) surrounding an origin (0 in this case)\n",
    "# with specified resolution and size. These kernels can represent geographic latitudes and longitudes or time intervals.\n",
    "\n",
    "# Generate a latitude kernel with a size of 4 and a resolution of 0.1 degrees.\n",
    "# The size parameter defines the total number of positions around the origin (0), including the origin for odd sizes.\n",
    "# Since the origin is 0 and the size is even, the origin will not be included in the output.\n",
    "lat_kernel = generate_equal_kernel(size=4, resolution=0.1, offset=0)\n",
    "print(\"Latitude Kernel (size=4, resolution=0.1):\", lat_kernel)\n",
    "\n",
    "# Generate a longitude kernel with a size of 5 and a resolution of 0.1 degrees.\n",
    "# Here, the size is odd, so the kernel includes the origin (0) and spreads equally around it.\n",
    "long_kernel = generate_equal_kernel(size=5, resolution=0.1, offset=0)\n",
    "print(\"Longitude Kernel (size=5, resolution=0.1):\", long_kernel)\n",
    "\n",
    "# Testing the Gaussian kernel generation\n",
    "# This demonstrates dividing the area under a Gaussian distribution curve into equal parts.\n",
    "# The size parameter specifies how many divisions to make, influencing the number of points generated.\n",
    "\n",
    "# Generate a temporal kernel with a size of 5, mean of 0, and a standard deviation of 15.\n",
    "# The 'datehour' flag converts the output into timedelta objects, making it useful for time-based calculations.\n",
    "# This can be particularly helpful for generating time intervals that follow a Gaussian distribution around a mean time point.\n",
    "t_kernel = generate_gaussian_kernel(size=5, mean=0, std_dev=15, datehour=True)\n",
    "print(\"Temporal Kernel (size=5, mean=0, std_dev=15):\", t_kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d6c92-deca-4bda-b575-0c4246106cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6506040-c795-4658-9095-b0bf1d352bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7afc43a7-6573-410b-ab85-4eed7876c496",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Query By Tabular\n",
    "\n",
    "Given: \n",
    "- df\n",
    "- xarray\n",
    "\n",
    "Wanted: \n",
    "- df with features in xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cda0e9-72a4-4c3a-b963-ff99fa4dea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from sample.retrieval import Xarray2Tabular\n",
    "\n",
    "grib_file_path = r'C:\\Datasets\\Zhejiang20-23RS\\ERA5_featureRanking1\\2021_Jan_filtered.nc'\n",
    "#dataset = xr.open_dataset(grib_file_path)\n",
    "#df=Xarray2Tabular(dataset,df)\n",
    "import pandas as pd\n",
    "#df.to_pickle(grib_file_path.replace(\".nc\",\"_tabular.pkl\"))    #to save the dataframe, df to 123.pkl\n",
    "#df = pd.read_pickle(grib_file_path.replace(\".nc\",\"_tabular.pkl\")) #to load 123.pkl back to the dataframe df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c13d226-c906-44c1-9524-59c0e74b67cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Negative_oxygen_ions</th>\n",
       "      <th>ELongtitude</th>\n",
       "      <th>NLatitude</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2021-10-14 09:00:00</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>121.23</td>\n",
       "      <td>29.07</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2021-10-12 13:00:00</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>121.23</td>\n",
       "      <td>29.07</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2021-10-03 10:00:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>121.23</td>\n",
       "      <td>29.07</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2021-10-09 23:00:00</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>121.23</td>\n",
       "      <td>29.07</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2021-10-10 18:00:00</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>121.23</td>\n",
       "      <td>29.07</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216876</th>\n",
       "      <td>2021-10-14 21:00:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>119.11</td>\n",
       "      <td>29.02</td>\n",
       "      <td>66.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217062</th>\n",
       "      <td>2021-10-04 20:00:00</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>119.11</td>\n",
       "      <td>29.02</td>\n",
       "      <td>66.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217430</th>\n",
       "      <td>2021-10-08 10:00:00</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>119.11</td>\n",
       "      <td>29.02</td>\n",
       "      <td>66.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217582</th>\n",
       "      <td>2021-10-15 23:00:00</td>\n",
       "      <td>4220.0</td>\n",
       "      <td>119.11</td>\n",
       "      <td>29.02</td>\n",
       "      <td>66.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217652</th>\n",
       "      <td>2021-10-12 18:00:00</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>119.11</td>\n",
       "      <td>29.02</td>\n",
       "      <td>66.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19881 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime  Negative_oxygen_ions  ELongtitude  NLatitude  \\\n",
       "67      2021-10-14 09:00:00                1010.0       121.23      29.07   \n",
       "69      2021-10-12 13:00:00                1030.0       121.23      29.07   \n",
       "81      2021-10-03 10:00:00                1000.0       121.23      29.07   \n",
       "121     2021-10-09 23:00:00                1030.0       121.23      29.07   \n",
       "171     2021-10-10 18:00:00                1030.0       121.23      29.07   \n",
       "...                     ...                   ...          ...        ...   \n",
       "1216876 2021-10-14 21:00:00                1000.0       119.11      29.02   \n",
       "1217062 2021-10-04 20:00:00                1880.0       119.11      29.02   \n",
       "1217430 2021-10-08 10:00:00                1930.0       119.11      29.02   \n",
       "1217582 2021-10-15 23:00:00                4220.0       119.11      29.02   \n",
       "1217652 2021-10-12 18:00:00                2040.0       119.11      29.02   \n",
       "\n",
       "         Height  \n",
       "67         34.5  \n",
       "69         34.5  \n",
       "81         34.5  \n",
       "121        34.5  \n",
       "171        34.5  \n",
       "...         ...  \n",
       "1216876    66.2  \n",
       "1217062    66.2  \n",
       "1217430    66.2  \n",
       "1217582    66.2  \n",
       "1217652    66.2  \n",
       "\n",
       "[19881 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d635cc9-3de0-459a-9ca2-54ab2a8d303f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Single Patch of Pixels with Given Coordinate\n",
    "\n",
    "Given: [time, lat, lon, xarr]\n",
    "\n",
    "Wanted: value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c80430fc-589c-48b6-997e-1cb39e087daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from sample.retrieval import generate_equal_kernel, generate_gaussian_kernel,Xarrayto0D,Xarrayto1D,Xarrayto2D,Xarrayto3D,get_patch_by_coordinate\n",
    "# Example usage (this code will not run here since the dataset file is not accessible)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from tqdm import tqdm \n",
    "from sample.retrieval import generate_equal_kernel, generate_gaussian_kernel\n",
    "\n",
    "\n",
    "time = datetime(2022, 5, 3, 13)\n",
    "latitude = 28.6895\n",
    "longitude = 121\n",
    "coor_dict = {\"time\": time, \"latitude\": latitude, \"longitude\": longitude}\n",
    "lat_kernel = generate_equal_kernel(size=4, resolution=0.1, offset=0)\n",
    "long_kernel = generate_equal_kernel(size=5, resolution=0.1, offset=0)\n",
    "t_kernel = generate_gaussian_kernel(size=5, mean=0, std_dev=15, datehour=True)\n",
    "\n",
    "#coor_df = pd.DataFrame([coor_dict] * 100)\n",
    "\n",
    "\n",
    "patch=Xarrayto0D(dataset,coor_dict , method=\"linear\")\n",
    "patch=Xarrayto1D(dataset,coor_dict , t_kernel, method=\"linear\")\n",
    "patch=Xarrayto2D(dataset,coor_dict , lat_kernel, long_kernel, method=\"linear\")\n",
    "patch=Xarrayto3D(dataset,coor_dict , t_kernel, lat_kernel, long_kernel,  method=\"linear\")\n",
    "patch = get_patch_by_coordinate(dataset, coor_dict={\"time\": time, \"latitude\": latitude, \"longitude\": longitude},\n",
    "                                kernel_dict={\"time\": t_kernel, \"latitude\": lat_kernel, \"longitude\": long_kernel},\n",
    "                                method=\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb17b3c9-d860-4e25-8465-e79514d7e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for 100 runs: 21.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52ab50d5-5a16-454c-b83d-b97289320648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get Patch(hdf5) of Pixels with Given Pandas\n",
    "\n",
    "Given:\n",
    "csv_path, time_range, dataset\n",
    "\n",
    "Wanted:\n",
    "hdf5 path, eg:\n",
    "Structure of C:\\Datasets\\Zhejiang20-23RS\\temp_training\\test_3d.h5:\r\n",
    " - Dataset 'data', Shape:eg. (500,m, n, c3), Dtype: float32\r\n",
    " - Dataset 'labels', Shape: (500,), Dtype: float\n",
    "32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce49703-2694-4a6a-a72e-45985e3be8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def save_ds_for_training(name: str ,save_dir: str, dataset: xr.Dataset, df: pd.DataFrame,\n",
    "                         dim: int, method: str,t_kernel: list=None,  lat_kernel: list=None,\n",
    "                         long_kernel: list=None, batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Extracts data patches from an xarray.Dataset and saves them into an HDF5 file\n",
    "    for training purposes, based on the specified dimensionality and method. This version\n",
    "    supports saving data in batches to efficiently manage large datasets.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): The directory path where the HDF5 file will be saved.\n",
    "        dataset (xr.Dataset): The dataset from which to extract data.\n",
    "        df (pd.DataFrame): DataFrame containing the coordinates and labels for extraction.\n",
    "        dim (int): The dimensionality of the data to extract (0D, 1D, 2D, 3D).\n",
    "        kernel_dict (dict): A dictionary with kernel_dict for extraction.\n",
    "        method (str): The interpolation method to use (\"linear\", \"nearest\", etc.).\n",
    "        batch_size (int): The number of data patches to process and save in each batch.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    hdf5_path = f\"{save_dir}//{name}.h5\"\n",
    "\n",
    "    # Prepare HDF5 file\n",
    "    with h5py.File(hdf5_path, 'w') as h5f:\n",
    "        data_patches = []\n",
    "        labels = []\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "            coor_dict = {\"time\": pd.to_datetime(row['Datetime']), \n",
    "                         \"latitude\": row['NLatitude'], \n",
    "                         \"longitude\": row['ELongtitude']}\n",
    "\n",
    "            if dim == 0:\n",
    "                patch = Xarrayto0D(dataset, coor_dict, method=method)\n",
    "            elif dim == 1:\n",
    "                patch = Xarrayto1D(dataset, coor_dict, t_kernel, method=method)\n",
    "            elif dim == 2:\n",
    "                patch = Xarrayto2D(dataset, coor_dict, lat_kernel, long_kernel, method=method)\n",
    "            elif dim == 3:\n",
    "                patch = Xarrayto3D(dataset, coor_dict, t_kernel, lat_kernel, long_kernel,  method=method)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported dimensionality.\")\n",
    "\n",
    "            data_patches.append(patch)\n",
    "            labels.append(row['Negative_oxygen_ions'])\n",
    "\n",
    "            # Save in batches\n",
    "            if (index + 1) % batch_size == 0 or (index + 1) == len(df):\n",
    "                data_patches_np = np.array(data_patches, dtype='float32')\n",
    "                labels_np = np.array(labels, dtype='float32')\n",
    "\n",
    "                if 'data' not in h5f:\n",
    "                    h5f.create_dataset('data', data=data_patches_np, maxshape=(None,) + data_patches_np.shape[1:], dtype='float32')\n",
    "                    h5f.create_dataset('labels', data=labels_np, maxshape=(None,), dtype='float32')\n",
    "                else:\n",
    "                    h5f['data'].resize((h5f['data'].shape[0] + data_patches_np.shape[0]), axis=0)\n",
    "                    h5f['data'][-data_patches_np.shape[0]:] = data_patches_np\n",
    "\n",
    "                    h5f['labels'].resize((h5f['labels'].shape[0] + labels_np.shape[0]), axis=0)\n",
    "                    h5f['labels'][-labels_np.shape[0]:] = labels_np\n",
    "\n",
    "                data_patches = []\n",
    "                labels = []\n",
    "\n",
    "    print(f\"Data saved to {hdf5_path}\")\n",
    "    return hdf5_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4828db-4c83-48ad-aba8-3a1ab9333b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"C:\\Datasets\\ZheJiang/merged_data.csv\"\n",
    "start_date = '2021-10-11'\n",
    "end_date = '2021-10-15'\n",
    "save_dir=r\"C:\\Datasets\\Zhejiang20-23RS\\ERA5_Training\"\n",
    "dataset=\"C:/Users/isxzl/OneDrive/Code/AutoGeo/data/saved_on_disk.nc\"\n",
    "lat_kernel = generate_equal_kernel(size=4, resolution=0.1, offset=0)\n",
    "long_kernel = generate_equal_kernel(size=5, resolution=0.1, offset=0)\n",
    "t_kernel = generate_gaussian_kernel(size=5, mean=0, std_dev=15, datehour=True)\n",
    "\n",
    "df=filter_dataframe_by_date(csv_path,start_date,end_date)\n",
    "\n",
    "h5_path=save_ds_for_training(\"211001-211020_1d\", save_dir,dataset,df,\n",
    "                     t_kernel=t_kernel,lat_kernel=lat_kernel, long_kernel=long_kernel,\n",
    "                     method=\"linear\",dim=1,batch_size=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75114405-8b92-4b77-b6fc-0a26442bb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of C:\\Datasets\\Zhejiang20-23RS\\ERA5_Training//211001-211020_1d.h5:\n",
      " - Dataset 'data', Shape: (1111, 10, 5), Dtype: float32\n",
      " - Dataset 'labels', Shape: (1111,), Dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoGeo\")\n",
    "from tqdm import tqdm \n",
    "from sample.unit_test import generate_example, generate_large_dataset, print_file_structure\t\n",
    "print_file_structure(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e6516-2b6e-47e1-92ea-b42d2d3b14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted_dims = {}\n",
    "# kernel_dict={\"time\": t_kernel, \"latitude\": lat_kernel, \"longitude\": long_kernel}\n",
    "# for dim, coord in coor_dict.items():\n",
    "#     kernel = kernel_dict.get(dim)\n",
    "#     if kernel:\n",
    "#         # Create a list of coordinates around the specified coordinate using the kernel\n",
    "#         adjusted_dims[dim] = [coord + k for k in kernel]\n",
    "# adjusted_dims        \n",
    "#patch = dataset.interp(adjusted_dims, method=method)\n",
    "# # import time\n",
    "\n",
    "# # Assume dataset, coor_dict, and method are already defined\n",
    "# start_time = time.time()   \n",
    "\n",
    "# for i in tqdm(range(100)):\n",
    "#     #Xarrayto1D(dataset,coor_dict , t_kernel, method=\"linear\") # 19.80s\n",
    "#     #Xarrayto2D(dataset,coor_dict , lat_kernel, long_kernel, method=\"linear\") # 19.91 seconds\n",
    "#     #Xarrayto3D(dataset,coor_dict , t_kernel, lat_kernel, long_kernel,  method=\"linear\") # 20.27 seconds\n",
    "#     #get_patch_by_coordinate(dataset, coor_dict,kernel_dict={\"time\": t_kernel, \"latitude\": lat_kernel, \"longitude\": long_kernel},\n",
    "#     #                            method=\"linear\") #21.64 seconds\n",
    "#     dataset.interp(adjusted_dims, method=\"nearest\") #21.24 seconds\n",
    "# end_time = time.time() \n",
    "# total_time = end_time - start_time\n",
    "# print(f\"Total time for 100 runs: {total_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
